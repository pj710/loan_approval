{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6e6bbb",
   "metadata": {},
   "source": [
    "# Phase 10: Testing & Validation\n",
    "\n",
    "## AI-Powered Mortgage Underwriting Assistant\n",
    "\n",
    "**Objective**: Comprehensive validation of the final model against success criteria, system integration testing, and error analysis.\n",
    "\n",
    "### Tasks:\n",
    "- **Task 10.1**: Model Validation - Final test set evaluation against success criteria\n",
    "- **Task 10.2**: System Testing - End-to-end API/Dashboard integration and performance testing\n",
    "- **Task 10.3**: Error Analysis - False positive/negative analysis and model limitations\n",
    "\n",
    "### Success Criteria:\n",
    "- AUC-ROC ‚â• 0.75\n",
    "- Precision ‚â• 0.80\n",
    "- Recall ‚â• 0.70\n",
    "- API Response Time < 500ms\n",
    "- Fairness: Demographic Parity Ratio ‚â• 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f66fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /Users/josiahgordor/Desktop/DSPortfolio/Projects/loan_approval\n",
      "Data Directory: /Users/josiahgordor/Desktop/DSPortfolio/Projects/loan_approval/data/processed\n",
      "Timestamp: 2026-02-17 13:38:58\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, classification_report,\n",
    "    average_precision_score, brier_score_loss\n",
    ")\n",
    "\n",
    "# Fairness\n",
    "from fairlearn.metrics import (\n",
    "    demographic_parity_ratio,\n",
    "    equalized_odds_ratio,\n",
    "    MetricFrame\n",
    ")\n",
    "\n",
    "# Set paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd0518",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 10.1: Model Validation\n",
    "\n",
    "### 10.1.1 Load Held-Out Test Set and Fair Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "print(f\"Test set size: {len(test_df):,} samples\")\n",
    "\n",
    "# Identify target and features\n",
    "target_col = 'loan_approved'\n",
    "if target_col not in test_df.columns:\n",
    "    target_col = 'action_taken_binary'\n",
    "    if target_col not in test_df.columns:\n",
    "        # Find target column\n",
    "        potential_targets = [c for c in test_df.columns if 'target' in c.lower() or 'approved' in c.lower() or 'action' in c.lower()]\n",
    "        print(f\"Potential target columns: {potential_targets}\")\n",
    "        target_col = potential_targets[0] if potential_targets else test_df.columns[-1]\n",
    "\n",
    "print(f\"Target column: {target_col}\")\n",
    "print(f\"Target distribution:\\n{test_df[target_col].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fair representation components\n",
    "fair_rep_dir = MODELS_DIR / 'fair_representation'\n",
    "fair_models_dir = MODELS_DIR / 'fair_models'\n",
    "\n",
    "# Load scaler\n",
    "scaler = joblib.load(fair_rep_dir / 'fair_scaler.pkl')\n",
    "print(\"‚úÖ Scaler loaded\")\n",
    "\n",
    "# Load encoder\n",
    "from tensorflow import keras\n",
    "try:\n",
    "    encoder = keras.models.load_model(fair_rep_dir / 'fair_encoder.keras')\n",
    "    print(\"‚úÖ Encoder loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Encoder load error: {e}\")\n",
    "    encoder = None\n",
    "\n",
    "# Load metadata\n",
    "with open(fair_rep_dir / 'fair_representation_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "selected_features = metadata.get('selected_features', [])[:metadata.get('input_dim', 32)]\n",
    "input_dim = metadata.get('input_dim', 32)\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Selected features: {len(selected_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b75e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all fair models for comparison\n",
    "fair_models = {}\n",
    "\n",
    "model_files = {\n",
    "    'XGB_Fair': 'xgb_fair.pkl',\n",
    "    'RF_Fair': 'rf_fair.pkl',\n",
    "    'LR_Fair': 'lr_fair.pkl',\n",
    "    'GLM_Fair': 'glm_fair.pkl'\n",
    "}\n",
    "\n",
    "for name, filename in model_files.items():\n",
    "    filepath = fair_models_dir / filename\n",
    "    if filepath.exists():\n",
    "        fair_models[name] = joblib.load(filepath)\n",
    "        print(f\"‚úÖ Loaded {name}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è {name} not found\")\n",
    "\n",
    "print(f\"\\nTotal models loaded: {len(fair_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "# Get feature columns (present in test_df and in selected_features)\n",
    "available_features = [f for f in selected_features if f in test_df.columns][:input_dim]\n",
    "\n",
    "# If not enough features, add more from test_df\n",
    "if len(available_features) < input_dim:\n",
    "    numeric_cols = test_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numeric_cols:\n",
    "        if col not in available_features and col != target_col:\n",
    "            available_features.append(col)\n",
    "        if len(available_features) >= input_dim:\n",
    "            break\n",
    "\n",
    "available_features = available_features[:input_dim]\n",
    "print(f\"Using {len(available_features)} features for testing\")\n",
    "\n",
    "# Prepare X and y\n",
    "X_test = test_df[available_features].fillna(0).values\n",
    "y_test = test_df[target_col].values\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform through fair representation pipeline\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "if encoder is not None:\n",
    "    X_test_latent = encoder.predict(X_test_scaled, verbose=0)\n",
    "    print(f\"X_test_latent shape: {X_test_latent.shape}\")\n",
    "else:\n",
    "    X_test_latent = X_test_scaled\n",
    "    print(\"Using scaled features (no encoder)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f3d4e",
   "metadata": {},
   "source": [
    "### 10.1.2 Final Evaluation Against Success Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define success criteria\n",
    "SUCCESS_CRITERIA = {\n",
    "    'AUC-ROC': 0.75,\n",
    "    'Precision': 0.80,\n",
    "    'Recall': 0.70,\n",
    "    'F1-Score': 0.70  # Derived from precision/recall requirements\n",
    "}\n",
    "\n",
    "print(\"Success Criteria:\")\n",
    "for metric, threshold in SUCCESS_CRITERIA.items():\n",
    "    print(f\"  {metric}: ‚â• {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models against success criteria\n",
    "results = []\n",
    "\n",
    "for model_name, model in fair_models.items():\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test_latent)\n",
    "    y_prob = model.predict_proba(X_test_latent)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Check against criteria\n",
    "    auc_pass = auc >= SUCCESS_CRITERIA['AUC-ROC']\n",
    "    precision_pass = precision >= SUCCESS_CRITERIA['Precision']\n",
    "    recall_pass = recall >= SUCCESS_CRITERIA['Recall']\n",
    "    all_pass = auc_pass and precision_pass and recall_pass\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'AUC-ROC': auc,\n",
    "        'AUC_Pass': '‚úÖ' if auc_pass else '‚ùå',\n",
    "        'Precision': precision,\n",
    "        'Prec_Pass': '‚úÖ' if precision_pass else '‚ùå',\n",
    "        'Recall': recall,\n",
    "        'Rec_Pass': '‚úÖ' if recall_pass else '‚ùå',\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'All_Criteria_Met': '‚úÖ PASS' if all_pass else '‚ùå FAIL'\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL EVALUATION AGAINST SUCCESS CRITERIA\")\n",
    "print(\"=\"*80)\n",
    "display(results_df.style.format({\n",
    "    'AUC-ROC': '{:.4f}',\n",
    "    'Precision': '{:.4f}',\n",
    "    'Recall': '{:.4f}',\n",
    "    'F1-Score': '{:.4f}',\n",
    "    'Accuracy': '{:.4f}'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics_to_plot = ['AUC-ROC', 'Precision', 'Recall']\n",
    "thresholds = [SUCCESS_CRITERIA['AUC-ROC'], SUCCESS_CRITERIA['Precision'], SUCCESS_CRITERIA['Recall']]\n",
    "\n",
    "for ax, metric, threshold in zip(axes, metrics_to_plot, thresholds):\n",
    "    values = results_df[metric].values\n",
    "    colors = ['green' if v >= threshold else 'red' for v in values]\n",
    "    \n",
    "    bars = ax.barh(results_df['Model'], values, color=colors, alpha=0.7)\n",
    "    ax.axvline(x=threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(f'{metric} vs Success Criteria')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'success_criteria_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d56c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model (recommended: XGB_Fair)\n",
    "best_model_name = 'XGB_Fair'\n",
    "best_model = fair_models.get(best_model_name, list(fair_models.values())[0])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Detailed evaluation\n",
    "y_pred_best = best_model.predict(X_test_latent)\n",
    "y_prob_best = best_model.predict_proba(X_test_latent)[:, 1]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Denied', 'Approved']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for best model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_best)\n",
    "auc_score = roc_auc_score(y_test, y_prob_best)\n",
    "\n",
    "axes[0].plot(fpr, tpr, 'b-', linewidth=2, label=f'{best_model_name} (AUC = {auc_score:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].axhline(y=0.70, color='green', linestyle=':', label='Recall Threshold (0.70)')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.2)\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curve - Final Model')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_prob_best)\n",
    "ap = average_precision_score(y_test, y_prob_best)\n",
    "\n",
    "axes[1].plot(recall_vals, precision_vals, 'g-', linewidth=2, label=f'{best_model_name} (AP = {ap:.4f})')\n",
    "axes[1].axhline(y=0.80, color='red', linestyle=':', label='Precision Threshold (0.80)')\n",
    "axes[1].axvline(x=0.70, color='blue', linestyle=':', label='Recall Threshold (0.70)')\n",
    "axes[1].fill_between(recall_vals, precision_vals, alpha=0.2, color='green')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve - Final Model')\n",
    "axes[1].legend(loc='lower left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'final_model_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf95d6e",
   "metadata": {},
   "source": [
    "### 10.1.3 Business Validation - Edge Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define business edge cases\n",
    "edge_cases = [\n",
    "    {\n",
    "        'name': 'High LTV, Low Income',\n",
    "        'loan_amount': 380000,\n",
    "        'property_value': 400000,  # LTV = 95%\n",
    "        'income': 50000,  # LTI = 7.6x\n",
    "        'interest_rate': 7.5,\n",
    "        'expected': 'Denied',\n",
    "        'reason': 'High risk - LTV > 90% and LTI > 6x'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Low LTV, High Income',\n",
    "        'loan_amount': 200000,\n",
    "        'property_value': 500000,  # LTV = 40%\n",
    "        'income': 150000,  # LTI = 1.3x\n",
    "        'interest_rate': 6.0,\n",
    "        'expected': 'Approved',\n",
    "        'reason': 'Low risk - strong equity and income'\n",
    "    },\n",
    "    {\n",
    "        'name': 'FHA First-Time Buyer',\n",
    "        'loan_amount': 250000,\n",
    "        'property_value': 270000,  # LTV = 92.6%\n",
    "        'income': 65000,  # LTI = 3.8x\n",
    "        'interest_rate': 6.5,\n",
    "        'is_fha_loan': True,\n",
    "        'expected': 'Approved',\n",
    "        'reason': 'FHA allows higher LTV for qualified buyers'\n",
    "    },\n",
    "    {\n",
    "        'name': 'VA Loan - Veteran',\n",
    "        'loan_amount': 400000,\n",
    "        'property_value': 400000,  # LTV = 100%\n",
    "        'income': 100000,  # LTI = 4x\n",
    "        'interest_rate': 5.5,\n",
    "        'is_va_loan': True,\n",
    "        'expected': 'Approved',\n",
    "        'reason': 'VA loans allow 100% LTV'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Jumbo Loan',\n",
    "        'loan_amount': 1500000,\n",
    "        'property_value': 2000000,  # LTV = 75%\n",
    "        'income': 400000,  # LTI = 3.75x\n",
    "        'interest_rate': 7.0,\n",
    "        'expected': 'Approved',\n",
    "        'reason': 'Strong metrics despite jumbo size'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Borderline Case',\n",
    "        'loan_amount': 300000,\n",
    "        'property_value': 375000,  # LTV = 80%\n",
    "        'income': 75000,  # LTI = 4x\n",
    "        'interest_rate': 6.75,\n",
    "        'expected': 'Moderate Risk',\n",
    "        'reason': 'On the edge - needs manual review'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(edge_cases)} business edge cases...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf353fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases against API (if available)\n",
    "API_URL = 'http://localhost:8000'\n",
    "\n",
    "def test_edge_case_api(case):\n",
    "    \"\"\"Test an edge case against the API.\"\"\"\n",
    "    payload = {\n",
    "        'loan_amount': case['loan_amount'],\n",
    "        'property_value': case['property_value'],\n",
    "        'income': case['income'],\n",
    "        'interest_rate': case['interest_rate'],\n",
    "        'loan_term': 360,\n",
    "        'is_fha_loan': case.get('is_fha_loan', False),\n",
    "        'is_va_loan': case.get('is_va_loan', False)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(f\"{API_URL}/predict\", json=payload, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {'error': f'Status {response.status_code}'}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Check if API is available\n",
    "api_available = False\n",
    "try:\n",
    "    health = requests.get(f\"{API_URL}/health\", timeout=2)\n",
    "    api_available = health.status_code == 200\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"API Available: {'‚úÖ Yes' if api_available else '‚ùå No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c527090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run edge case tests\n",
    "edge_case_results = []\n",
    "\n",
    "for case in edge_cases:\n",
    "    result = {\n",
    "        'Case': case['name'],\n",
    "        'LTV': case['loan_amount'] / case['property_value'] * 100,\n",
    "        'LTI': case['loan_amount'] / case['income'],\n",
    "        'Expected': case['expected'],\n",
    "        'Reason': case['reason']\n",
    "    }\n",
    "    \n",
    "    if api_available:\n",
    "        api_result = test_edge_case_api(case)\n",
    "        if 'error' not in api_result:\n",
    "            result['Prediction'] = api_result.get('prediction', 'Unknown')\n",
    "            result['Probability'] = api_result.get('probability', 0)\n",
    "            result['Risk_Level'] = api_result.get('risk_level', 'Unknown')\n",
    "            result['Match'] = '‚úÖ' if case['expected'] in [result['Prediction'], result['Risk_Level']] else '‚ö†Ô∏è'\n",
    "        else:\n",
    "            result['Prediction'] = 'API Error'\n",
    "            result['Match'] = '‚ùì'\n",
    "    else:\n",
    "        result['Prediction'] = 'API Unavailable'\n",
    "        result['Match'] = '‚ùì'\n",
    "    \n",
    "    edge_case_results.append(result)\n",
    "\n",
    "edge_df = pd.DataFrame(edge_case_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDGE CASE VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "display(edge_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54010b1d",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 10.2: System Testing\n",
    "\n",
    "### 10.2.1 End-to-End Integration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0236c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration test suite\n",
    "def run_integration_tests():\n",
    "    \"\"\"Run end-to-end integration tests.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Test 1: Health endpoint\n",
    "    test_name = \"Health Endpoint\"\n",
    "    try:\n",
    "        resp = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        passed = resp.status_code == 200 and 'status' in resp.json()\n",
    "        results.append({'Test': test_name, 'Status': '‚úÖ PASS' if passed else '‚ùå FAIL', 'Details': resp.json().get('status', 'N/A')})\n",
    "    except Exception as e:\n",
    "        results.append({'Test': test_name, 'Status': '‚ùå FAIL', 'Details': str(e)[:50]})\n",
    "    \n",
    "    # Test 2: Predict endpoint - valid input\n",
    "    test_name = \"Predict - Valid Input\"\n",
    "    try:\n",
    "        payload = {'loan_amount': 250000, 'property_value': 300000, 'income': 80000, 'interest_rate': 6.5, 'loan_term': 360}\n",
    "        resp = requests.post(f\"{API_URL}/predict\", json=payload, timeout=10)\n",
    "        passed = resp.status_code == 200 and 'prediction' in resp.json()\n",
    "        results.append({'Test': test_name, 'Status': '‚úÖ PASS' if passed else '‚ùå FAIL', 'Details': f\"Returned: {resp.json().get('prediction', 'N/A')}\"})\n",
    "    except Exception as e:\n",
    "        results.append({'Test': test_name, 'Status': '‚ùå FAIL', 'Details': str(e)[:50]})\n",
    "    \n",
    "    # Test 3: Predict endpoint - invalid input\n",
    "    test_name = \"Predict - Invalid Input (Validation)\"\n",
    "    try:\n",
    "        payload = {'loan_amount': -1000, 'property_value': 300000, 'income': 80000, 'interest_rate': 6.5, 'loan_term': 360}\n",
    "        resp = requests.post(f\"{API_URL}/predict\", json=payload, timeout=5)\n",
    "        passed = resp.status_code == 422  # Validation error expected\n",
    "        results.append({'Test': test_name, 'Status': '‚úÖ PASS' if passed else '‚ùå FAIL', 'Details': f\"Status: {resp.status_code}\"})\n",
    "    except Exception as e:\n",
    "        results.append({'Test': test_name, 'Status': '‚ùå FAIL', 'Details': str(e)[:50]})\n",
    "    \n",
    "    # Test 4: Explain endpoint\n",
    "    test_name = \"Explain Endpoint\"\n",
    "    try:\n",
    "        payload = {'loan_amount': 250000, 'property_value': 300000, 'income': 80000, 'interest_rate': 6.5, 'loan_term': 360}\n",
    "        resp = requests.post(f\"{API_URL}/explain\", json=payload, timeout=30)\n",
    "        passed = resp.status_code == 200 and 'explanation_text' in resp.json()\n",
    "        results.append({'Test': test_name, 'Status': '‚úÖ PASS' if passed else '‚ùå FAIL', 'Details': 'Explanation returned' if passed else f\"Status: {resp.status_code}\"})\n",
    "    except Exception as e:\n",
    "        results.append({'Test': test_name, 'Status': '‚ùå FAIL', 'Details': str(e)[:50]})\n",
    "    \n",
    "    # Test 5: Batch predict endpoint\n",
    "    test_name = \"Batch Predict Endpoint\"\n",
    "    try:\n",
    "        payload = {'applications': [\n",
    "            {'loan_amount': 250000, 'property_value': 300000, 'income': 80000, 'interest_rate': 6.5, 'loan_term': 360},\n",
    "            {'loan_amount': 400000, 'property_value': 500000, 'income': 120000, 'interest_rate': 7.0, 'loan_term': 360}\n",
    "        ]}\n",
    "        resp = requests.post(f\"{API_URL}/batch/predict\", json=payload, timeout=20)\n",
    "        passed = resp.status_code == 200 and resp.json().get('total_processed') == 2\n",
    "        results.append({'Test': test_name, 'Status': '‚úÖ PASS' if passed else '‚ùå FAIL', 'Details': f\"Processed: {resp.json().get('total_processed', 0)}\"})\n",
    "    except Exception as e:\n",
    "        results.append({'Test': test_name, 'Status': '‚ùå FAIL', 'Details': str(e)[:50]})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if api_available:\n",
    "    integration_results = run_integration_tests()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTEGRATION TEST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    display(integration_results)\n",
    "    \n",
    "    passed_count = (integration_results['Status'].str.contains('PASS')).sum()\n",
    "    total_count = len(integration_results)\n",
    "    print(f\"\\nPassed: {passed_count}/{total_count}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è API not available. Start API with: uvicorn src.api.main:app --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac29a4",
   "metadata": {},
   "source": [
    "### 10.2.2 Performance Testing (Latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4932c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance testing\n",
    "def run_performance_tests(n_requests=50):\n",
    "    \"\"\"Run performance/latency tests.\"\"\"\n",
    "    latencies = []\n",
    "    payload = {'loan_amount': 250000, 'property_value': 300000, 'income': 80000, 'interest_rate': 6.5, 'loan_term': 360}\n",
    "    \n",
    "    print(f\"Running {n_requests} requests...\")\n",
    "    \n",
    "    for i in range(n_requests):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            resp = requests.post(f\"{API_URL}/predict\", json=payload, timeout=10)\n",
    "            elapsed_ms = (time.time() - start) * 1000\n",
    "            if resp.status_code == 200:\n",
    "                latencies.append(elapsed_ms)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Completed {i + 1}/{n_requests}\")\n",
    "    \n",
    "    return latencies\n",
    "\n",
    "LATENCY_THRESHOLD_MS = 500  # Success criteria\n",
    "\n",
    "if api_available:\n",
    "    latencies = run_performance_tests(n_requests=50)\n",
    "    \n",
    "    if latencies:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PERFORMANCE TEST RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        mean_latency = np.mean(latencies)\n",
    "        p50 = np.percentile(latencies, 50)\n",
    "        p95 = np.percentile(latencies, 95)\n",
    "        p99 = np.percentile(latencies, 99)\n",
    "        max_latency = np.max(latencies)\n",
    "        \n",
    "        print(f\"\\nLatency Statistics (ms):\")\n",
    "        print(f\"  Mean:    {mean_latency:.2f} ms {'‚úÖ' if mean_latency < LATENCY_THRESHOLD_MS else '‚ùå'}\")\n",
    "        print(f\"  P50:     {p50:.2f} ms\")\n",
    "        print(f\"  P95:     {p95:.2f} ms {'‚úÖ' if p95 < LATENCY_THRESHOLD_MS else '‚ùå'}\")\n",
    "        print(f\"  P99:     {p99:.2f} ms\")\n",
    "        print(f\"  Max:     {max_latency:.2f} ms\")\n",
    "        print(f\"\\nThreshold: < {LATENCY_THRESHOLD_MS} ms\")\n",
    "        print(f\"Result: {'‚úÖ PASS' if p95 < LATENCY_THRESHOLD_MS else '‚ùå FAIL'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è API not available for performance testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency distribution\n",
    "if api_available and latencies:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(latencies, bins=20, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "    axes[0].axvline(x=LATENCY_THRESHOLD_MS, color='red', linestyle='--', linewidth=2, label=f'Threshold ({LATENCY_THRESHOLD_MS}ms)')\n",
    "    axes[0].axvline(x=np.mean(latencies), color='green', linestyle='-', linewidth=2, label=f'Mean ({np.mean(latencies):.1f}ms)')\n",
    "    axes[0].set_xlabel('Latency (ms)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('API Response Latency Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot(latencies, vert=True)\n",
    "    axes[1].axhline(y=LATENCY_THRESHOLD_MS, color='red', linestyle='--', linewidth=2, label=f'Threshold ({LATENCY_THRESHOLD_MS}ms)')\n",
    "    axes[1].set_ylabel('Latency (ms)')\n",
    "    axes[1].set_title('Latency Box Plot')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'performance_test_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48acb6a1",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 10.3: Error Analysis\n",
    "\n",
    "### 10.3.1 Confusion Matrix Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix analysis\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Confusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (TN):  {tn:,} - Correctly denied bad applications\")\n",
    "print(f\"  False Positives (FP): {fp:,} - Bad loans approved (Type I Error) ‚ö†Ô∏è\")\n",
    "print(f\"  False Negatives (FN): {fn:,} - Good borrowers denied (Type II Error) ‚ö†Ô∏è\")\n",
    "print(f\"  True Positives (TP):  {tp:,} - Correctly approved good applications\")\n",
    "\n",
    "# Error rates\n",
    "fpr_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "fnr_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "print(f\"\\nError Rates:\")\n",
    "print(f\"  False Positive Rate: {fpr_rate:.2%}\")\n",
    "print(f\"  False Negative Rate: {fnr_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Denied', 'Approved'],\n",
    "            yticklabels=['Denied', 'Approved'],\n",
    "            ax=ax, cbar=True)\n",
    "\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name}', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d52b7",
   "metadata": {},
   "source": [
    "### 10.3.2 False Positive Analysis (Bad Loans Approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102cfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify false positives\n",
    "fp_mask = (y_test == 0) & (y_pred_best == 1)\n",
    "fp_indices = np.where(fp_mask)[0]\n",
    "\n",
    "print(f\"False Positives (Bad loans approved): {len(fp_indices):,}\")\n",
    "print(f\"Percentage of test set: {len(fp_indices) / len(y_test) * 100:.2f}%\")\n",
    "\n",
    "# Analyze characteristics of FPs\n",
    "if len(fp_indices) > 0:\n",
    "    fp_df = test_df.iloc[fp_indices].copy()\n",
    "    fp_df['probability'] = y_prob_best[fp_indices]\n",
    "    \n",
    "    print(f\"\\nFalse Positive Characteristics:\")\n",
    "    \n",
    "    # Check for common features\n",
    "    numeric_features = ['loan_amount', 'property_value', 'income', 'interest_rate', \n",
    "                        'loan_to_income_ratio', 'loan_to_value_ratio']\n",
    "    \n",
    "    for feat in numeric_features:\n",
    "        if feat in fp_df.columns:\n",
    "            fp_mean = fp_df[feat].mean()\n",
    "            overall_mean = test_df[feat].mean()\n",
    "            diff_pct = (fp_mean - overall_mean) / overall_mean * 100 if overall_mean != 0 else 0\n",
    "            print(f\"  {feat}: FP Mean = {fp_mean:.2f}, Overall Mean = {overall_mean:.2f} ({diff_pct:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384319bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FP probability distribution\n",
    "if len(fp_indices) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Probability distribution of FPs\n",
    "    axes[0].hist(y_prob_best[fp_indices], bins=20, color='red', alpha=0.7, edgecolor='white', label='False Positives')\n",
    "    axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    axes[0].set_xlabel('Predicted Probability')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Probability Distribution of False Positives')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Compare TP vs FP probabilities\n",
    "    tp_mask = (y_test == 1) & (y_pred_best == 1)\n",
    "    axes[1].hist(y_prob_best[tp_mask], bins=20, color='green', alpha=0.5, label='True Positives', edgecolor='white')\n",
    "    axes[1].hist(y_prob_best[fp_mask], bins=20, color='red', alpha=0.5, label='False Positives', edgecolor='white')\n",
    "    axes[1].set_xlabel('Predicted Probability')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('TP vs FP Probability Comparison')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'false_positive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136eb6ae",
   "metadata": {},
   "source": [
    "### 10.3.3 False Negative Analysis (Good Borrowers Denied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify false negatives\n",
    "fn_mask = (y_test == 1) & (y_pred_best == 0)\n",
    "fn_indices = np.where(fn_mask)[0]\n",
    "\n",
    "print(f\"False Negatives (Good borrowers denied): {len(fn_indices):,}\")\n",
    "print(f\"Percentage of test set: {len(fn_indices) / len(y_test) * 100:.2f}%\")\n",
    "\n",
    "# Analyze characteristics of FNs\n",
    "if len(fn_indices) > 0:\n",
    "    fn_df = test_df.iloc[fn_indices].copy()\n",
    "    fn_df['probability'] = y_prob_best[fn_indices]\n",
    "    \n",
    "    print(f\"\\nFalse Negative Characteristics:\")\n",
    "    \n",
    "    for feat in numeric_features:\n",
    "        if feat in fn_df.columns:\n",
    "            fn_mean = fn_df[feat].mean()\n",
    "            overall_mean = test_df[feat].mean()\n",
    "            diff_pct = (fn_mean - overall_mean) / overall_mean * 100 if overall_mean != 0 else 0\n",
    "            print(f\"  {feat}: FN Mean = {fn_mean:.2f}, Overall Mean = {overall_mean:.2f} ({diff_pct:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FN probability distribution\n",
    "if len(fn_indices) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Probability distribution of FNs\n",
    "    axes[0].hist(y_prob_best[fn_indices], bins=20, color='orange', alpha=0.7, edgecolor='white', label='False Negatives')\n",
    "    axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "    axes[0].set_xlabel('Predicted Probability')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Probability Distribution of False Negatives')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Compare TN vs FN probabilities\n",
    "    tn_mask = (y_test == 0) & (y_pred_best == 0)\n",
    "    axes[1].hist(y_prob_best[tn_mask], bins=20, color='blue', alpha=0.5, label='True Negatives', edgecolor='white')\n",
    "    axes[1].hist(y_prob_best[fn_mask], bins=20, color='orange', alpha=0.5, label='False Negatives', edgecolor='white')\n",
    "    axes[1].set_xlabel('Predicted Probability')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('TN vs FN Probability Comparison')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'false_negative_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b54aa7",
   "metadata": {},
   "source": [
    "### 10.3.4 Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different decision thresholds\n",
    "thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "\n",
    "threshold_results = []\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_prob_best >= thresh).astype(int)\n",
    "    \n",
    "    prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    \n",
    "    cm_t = confusion_matrix(y_test, y_pred_thresh)\n",
    "    if cm_t.shape == (2, 2):\n",
    "        tn_t, fp_t, fn_t, tp_t = cm_t.ravel()\n",
    "    else:\n",
    "        tn_t, fp_t, fn_t, tp_t = 0, 0, 0, 0\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': thresh,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1': f1,\n",
    "        'FP': fp_t,\n",
    "        'FN': fn_t,\n",
    "        'Meets_Criteria': prec >= 0.80 and rec >= 0.70\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "print(\"Threshold Analysis:\")\n",
    "display(threshold_df.style.format({'Threshold': '{:.2f}', 'Precision': '{:.4f}', 'Recall': '{:.4f}', 'F1': '{:.4f}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold trade-offs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Precision, Recall, F1 vs Threshold\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Precision'], 'b-', marker='o', label='Precision')\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Recall'], 'g-', marker='s', label='Recall')\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['F1'], 'r-', marker='^', label='F1')\n",
    "axes[0].axhline(y=0.80, color='blue', linestyle=':', alpha=0.5, label='Precision Target (0.80)')\n",
    "axes[0].axhline(y=0.70, color='green', linestyle=':', alpha=0.5, label='Recall Target (0.70)')\n",
    "axes[0].set_xlabel('Decision Threshold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Metrics vs Decision Threshold')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# FP and FN vs Threshold\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['FP'], 'r-', marker='o', label='False Positives')\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['FN'], 'orange', marker='s', label='False Negatives')\n",
    "axes[1].set_xlabel('Decision Threshold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Error Count vs Decision Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76a904",
   "metadata": {},
   "source": [
    "### 10.3.5 Model Limitations Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c68141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document model limitations\n",
    "limitations = {\n",
    "    \"Data Limitations\": [\n",
    "        \"Training data limited to NJ, NY, PA, CT states (may not generalize to other regions)\",\n",
    "        \"2024 HMDA data only - market conditions change over time\",\n",
    "        \"Missing DTI ratio data required imputation/estimation\",\n",
    "        \"Owner-occupied purchase loans only (not refinance or investment)\"\n",
    "    ],\n",
    "    \"Model Limitations\": [\n",
    "        f\"False Positive Rate: {fpr_rate:.2%} - Some bad loans still approved\",\n",
    "        f\"False Negative Rate: {fnr_rate:.2%} - Some qualified borrowers denied\",\n",
    "        \"Fair representation encoding may lose some predictive signal\",\n",
    "        \"Model trained on historical approvals (may perpetuate past biases)\"\n",
    "    ],\n",
    "    \"Operational Limitations\": [\n",
    "        \"Requires all input features - missing data reduces accuracy\",\n",
    "        \"API response includes SHAP explanations (adds latency for /explain)\",\n",
    "        \"Model should be retrained periodically as market conditions change\",\n",
    "        \"Not a replacement for human underwriter review on edge cases\"\n",
    "    ],\n",
    "    \"Fairness Considerations\": [\n",
    "        \"Fairness metrics monitored for race, ethnicity, sex\",\n",
    "        \"Some disparity may remain even with fair representation\",\n",
    "        \"Intersectional fairness not fully evaluated\",\n",
    "        \"Should be used as decision support, not sole decision maker\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL LIMITATIONS DOCUMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category, items in limitations.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ‚Ä¢ {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9752b9",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Testing & Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': best_model_name,\n",
    "    'test_set_size': len(y_test),\n",
    "    'success_criteria': {\n",
    "        'AUC_threshold': 0.75,\n",
    "        'AUC_achieved': float(roc_auc_score(y_test, y_prob_best)),\n",
    "        'AUC_pass': bool(roc_auc_score(y_test, y_prob_best) >= 0.75),\n",
    "        'Precision_threshold': 0.80,\n",
    "        'Precision_achieved': float(precision_score(y_test, y_pred_best)),\n",
    "        'Precision_pass': bool(precision_score(y_test, y_pred_best) >= 0.80),\n",
    "        'Recall_threshold': 0.70,\n",
    "        'Recall_achieved': float(recall_score(y_test, y_pred_best)),\n",
    "        'Recall_pass': bool(recall_score(y_test, y_pred_best) >= 0.70)\n",
    "    },\n",
    "    'error_analysis': {\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'fp_rate': float(fpr_rate),\n",
    "        'fn_rate': float(fnr_rate)\n",
    "    },\n",
    "    'performance': {\n",
    "        'mean_latency_ms': float(np.mean(latencies)) if api_available and latencies else None,\n",
    "        'p95_latency_ms': float(np.percentile(latencies, 95)) if api_available and latencies else None,\n",
    "        'latency_pass': bool(np.percentile(latencies, 95) < 500) if api_available and latencies else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = RESULTS_DIR / f\"validation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 10: TESTING & VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_criteria_pass = all([\n",
    "    summary['success_criteria']['AUC_pass'],\n",
    "    summary['success_criteria']['Precision_pass'],\n",
    "    summary['success_criteria']['Recall_pass']\n",
    "])\n",
    "\n",
    "print(f\"\\n‚úÖ Model: {best_model_name}\")\n",
    "print(f\"\\nüìä Success Criteria:\")\n",
    "print(f\"   AUC-ROC:   {summary['success_criteria']['AUC_achieved']:.4f} (‚â•0.75) {'‚úÖ' if summary['success_criteria']['AUC_pass'] else '‚ùå'}\")\n",
    "print(f\"   Precision: {summary['success_criteria']['Precision_achieved']:.4f} (‚â•0.80) {'‚úÖ' if summary['success_criteria']['Precision_pass'] else '‚ùå'}\")\n",
    "print(f\"   Recall:    {summary['success_criteria']['Recall_achieved']:.4f} (‚â•0.70) {'‚úÖ' if summary['success_criteria']['Recall_pass'] else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Error Analysis:\")\n",
    "print(f\"   False Positives: {fp:,} ({fpr_rate:.2%})\")\n",
    "print(f\"   False Negatives: {fn:,} ({fnr_rate:.2%})\")\n",
    "\n",
    "if api_available and latencies:\n",
    "    print(f\"\\n‚è±Ô∏è Performance:\")\n",
    "    print(f\"   Mean Latency: {summary['performance']['mean_latency_ms']:.1f}ms\")\n",
    "    print(f\"   P95 Latency:  {summary['performance']['p95_latency_ms']:.1f}ms (<500ms) {'‚úÖ' if summary['performance']['latency_pass'] else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL RESULT: {'‚úÖ ALL CRITERIA MET' if all_criteria_pass else '‚ùå SOME CRITERIA NOT MET'}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds4b_101p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
